{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Web Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos classes para todos os métodos de Web Scrapping pois compactam bem o código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e34d184830f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mURL_PATTERN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'{BASE_URL}{CATEGORY}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mspider\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mScrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mURL_PATTERN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent_parser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mspider\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkickstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-e34d184830f6>\u001b[0m in \u001b[0;36mkickstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mkickstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscrape_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl_pattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-e34d184830f6>\u001b[0m in \u001b[0;36mscrape_url\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"fail. error code: {response.status_code}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-e34d184830f6>\u001b[0m in \u001b[0;36mparser\u001b[1;34m(content)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m#observamos que poderíamos reduzir a área de busca na sopa para evitar retornar lixo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mquoteshtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'h2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mquoteshtml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquoteshtml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "class Scrapper:\n",
    "    def __init__(self, url_pattern, content_parser=None):\n",
    "        self.url_pattern = url_pattern\n",
    "        self.content_parser = content_parser\n",
    "\n",
    "    def scrape_url(self, url):\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code < 300:\n",
    "            result = self.content_parser(response.content)[1:]\n",
    "            self.output_results(result)\n",
    "        else: print(f\"fail. error code: {response.status_code}\")\n",
    "\n",
    "    #para cada categoria do site criamos um arquivo diferente, inclusive para a categoria geral que reúne todas\n",
    "    def output_results(self, r):\n",
    "        if CATEGORY == '': file = open(f'omelete-noticias-geral.csv','w+')\n",
    "        else: file = open(f'omelete-noticias-{CATEGORY[9:]}.csv','w+')\n",
    "        for item in r:\n",
    "            file.write(f'{item}\\n')\n",
    "        file.close()\n",
    "\n",
    "    def kickstart(self):\n",
    "        self.scrape_url(self.url_pattern)\n",
    "\n",
    "\n",
    "#observamos que poderíamos reduzir a área de busca na sopa para evitar retornar lixo\n",
    "def parser(content):\n",
    "    soup = list(bs(content).children)[14]\n",
    "    quoteshtml = soup.find_all('h2')\n",
    "    return [quoteshtml[k].get_text() for k in range(len(quoteshtml))]\n",
    "\n",
    "#utilizamos uma url base que era comum a todas as cinco categorias que buscamos\n",
    "BASE_URL = 'https://www.omelete.com.br/criticas'\n",
    "CATEGORY_LIST = ['', '?section=filmes', '?section=series-tv', '?section=quadrinhos', '?section=musica']\n",
    "URL_PATTERN = ''\n",
    "\n",
    "#utilizamos um looping para rodar por todas as categorias\n",
    "for CATEGORY in CATEGORY_LIST:\n",
    "    URL_PATTERN = f'{BASE_URL}{CATEGORY}'\n",
    "    spider = Scrapper(URL_PATTERN, content_parser=parser)\n",
    "    spider.kickstart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Web Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NavigableString' object has no attribute 'children'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-8d2cc7a7cefc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[0mspider\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mScrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBASE_URL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent_parser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[0mspider\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkickstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-8d2cc7a7cefc>\u001b[0m in \u001b[0;36mkickstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mkickstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscrape_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl_pattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m#reduzimos a área de busca na sopa mas ainda assim percebemos que a lista carregava lixo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-8d2cc7a7cefc>\u001b[0m in \u001b[0;36mscrape_url\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"fail. error code: {response.status_code}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-8d2cc7a7cefc>\u001b[0m in \u001b[0;36mparser\u001b[1;34m(content)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m#também procuramos deixar todas as informações numéricas, incluindo porcentagens, em variáveis apenas numéricas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[0minfos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m#cortamos os 13 espaços em branco no começo do título do filme os últimos 7 caracteres referentes ao ano\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    740\u001b[0m             raise AttributeError(\n\u001b[0;32m    741\u001b[0m                 \"'%s' object has no attribute '%s'\" % (\n\u001b[1;32m--> 742\u001b[1;33m                     self.__class__.__name__, attr))\n\u001b[0m\u001b[0;32m    743\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0moutput_ready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"minimal\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NavigableString' object has no attribute 'children'"
     ]
    }
   ],
   "source": [
    "class Scrapper:\n",
    "    def __init__(self, url_pattern, content_parser=None):\n",
    "        self.url_pattern = url_pattern\n",
    "        self.content_parser = content_parser\n",
    "\n",
    "    def scrape_url(self, url):\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code < 300:\n",
    "            result = self.content_parser(response.content)\n",
    "            self.output_results(result)\n",
    "        else: print(f\"fail. error code: {response.status_code}\")\n",
    "\n",
    "    def output_results(self, r):\n",
    "        file = open('top100-movies-rt.csv', 'w+')\n",
    "        file.write(f'ranking,rating,movie_name,release_year,total_reviews')\n",
    "        file.write('\\n')\n",
    "        k = 0\n",
    "        for minilist in r:\n",
    "            k += 1\n",
    "            file.write(f'{k},{minilist[0]},{minilist[1]},{minilist[2]},{minilist[3]}\\n')\n",
    "        file.close()\n",
    "\n",
    "    def kickstart(self):\n",
    "        self.scrape_url(self.url_pattern)\n",
    "\n",
    "#reduzimos a área de busca na sopa mas ainda assim percebemos que a lista carregava lixo\n",
    "#por este motivo, limitamos o retorno aos elementos de índice 1 ao -20\n",
    "#desta forma retornamos exatamente a lista de 100 filmes\n",
    "#também procuramos deixar todas as informações numéricas, incluindo porcentagens, em variáveis apenas numéricas\n",
    "def parser(content):\n",
    "    soup = list(list(bs(content).children)[2].children)[3]\n",
    "    infos = soup.find_all('tr', class_='')[1:-20]\n",
    "    #cortamos os 13 espaços em branco no começo do título do filme os últimos 7 caracteres referentes ao ano\n",
    "    #colocamos o ano em uma coluna separada\n",
    "    return [[infos[k].find_all('span', class_='tMeterScore')[0].get_text()[1:].strip('%'),\n",
    "            infos[k].find_all('a', class_='unstyled articleLink')[0].get_text()[13:-7],\n",
    "            infos[k].find_all('a', class_='unstyled articleLink')[0].get_text().split(' ')[-1].strip('()'),\n",
    "            infos[k].find_all('td', class_='right hidden-xs')[0].get_text()] for k in range(len(infos))]\n",
    "\n",
    "BASE_URL = 'https://www.rottentomatoes.com/top/bestofrt/'\n",
    "\n",
    "spider = Scrapper(BASE_URL, content_parser=parser)\n",
    "spider.kickstart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Web Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NavigableString' object has no attribute 'children'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-d006b42aef0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mURL_PATTERN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'{BASE_URL}{CATEGORY}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mspider\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mScrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mURL_PATTERN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent_parser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[0mspider\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkickstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-d006b42aef0c>\u001b[0m in \u001b[0;36mkickstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mkickstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscrape_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl_pattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m#reduzimos a área de busca da sopa para evitar retorno de lixo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-d006b42aef0c>\u001b[0m in \u001b[0;36mscrape_url\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"fail. error code: {response.status_code}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-d006b42aef0c>\u001b[0m in \u001b[0;36mparser\u001b[1;34m(content)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m#percebemos que para três categorias específicas havia um valor a mais e precisamos fazer um retorno diferente\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m26\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mstats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'td'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mCATEGORY\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Thief'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Dragoon'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Mime'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    740\u001b[0m             raise AttributeError(\n\u001b[0;32m    741\u001b[0m                 \"'%s' object has no attribute '%s'\" % (\n\u001b[1;32m--> 742\u001b[1;33m                     self.__class__.__name__, attr))\n\u001b[0m\u001b[0;32m    743\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0moutput_ready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"minimal\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NavigableString' object has no attribute 'children'"
     ]
    }
   ],
   "source": [
    "class Scrapper:\n",
    "    def __init__(self, url_pattern, content_parser=None):\n",
    "        self.url_pattern = url_pattern\n",
    "        self.content_parser = content_parser\n",
    "\n",
    "    def scrape_url(self, url):\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code < 300:\n",
    "            result = self.content_parser(response.content)\n",
    "            self.output_results(result)\n",
    "        else: print(f\"fail. error code: {response.status_code}\")\n",
    "\n",
    "    #como aqui precisávamos limpar as informações mais a fundo preferimos criar uma lista antes de imprimir\n",
    "    #também adicionamos a categoria na lista para termos o nome da personagem nos dados\n",
    "    def output_results(self, r):\n",
    "        infos = r\n",
    "        infos.append(CATEGORY)\n",
    "        classes_info.append(infos)\n",
    "\n",
    "    def kickstart(self):\n",
    "        self.scrape_url(self.url_pattern)\n",
    "\n",
    "#reduzimos a área de busca da sopa para evitar retorno de lixo\n",
    "#percebemos que para três categorias específicas havia um valor a mais e precisamos fazer um retorno diferente\n",
    "def parser(content):\n",
    "    soup = list(list(list(bs(content).children)[2].children)[3].children)[26]\n",
    "    stats = soup.find_all('td')\n",
    "    if CATEGORY not in ['Thief', 'Dragoon', 'Mime']: return [item.get_text().strip(' ').strip('\\n') for item in stats[5:13]]\n",
    "    else: return [item.get_text().strip(' ').strip('\\n') for item in stats[6:14]]\n",
    "\n",
    "classes_info = []\n",
    "#utilizamos uma url base que era comum a todas as categorias que iríamos buscar\n",
    "BASE_URL = 'https://finalfantasy.fandom.com/wiki/'\n",
    "CATEGORY_LIST = ['Squire', 'Chemist', 'Knight', 'Archer', 'White_Mage',\n",
    "                 'Black_Mage', 'Monk', 'Thief', 'Mystic', 'Time_Mage',\n",
    "                 'Geomancer', 'Dragoon', 'Orator', 'Summoner', 'Samurai',\n",
    "                 'Ninja', 'Arithmetician', 'Dancer', 'Bard', 'Mime',\n",
    "                 'Dark_Knight', 'Onion_Knight', 'Holy_Knight', 'Machinist', 'Skyseer',\n",
    "                 'Netherseer', 'Divine_Knight', 'Sword_Saint', 'Templar', 'Dragonkin',\n",
    "                 'Soldier', 'Sky_Pirate', 'Game_Hunter']\n",
    "\n",
    "#utilizamos um looping para rodar todas as categorias que queríamos\n",
    "#mas percebemos que especificamente para 5 categorias o link não seguia o padrão exato dos outros e\n",
    "#precisamos fazer um retorno diferente\n",
    "#por precaução, como eram mais de 30 acessos, colocamos um intervalo de 1 segundo para evitar quaisquer problemas\n",
    "for CATEGORY in CATEGORY_LIST:\n",
    "    if CATEGORY not in ['Arithmetician', 'Skyseer', 'Netherseer', 'Dragonkin', 'Game_Hunter']: URL_PATTERN = f'{BASE_URL}{CATEGORY}_(Tactics)'\n",
    "    else: URL_PATTERN = f'{BASE_URL}{CATEGORY}'\n",
    "    spider = Scrapper(URL_PATTERN, content_parser=parser)\n",
    "    spider.kickstart()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-2343acdef771>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#ao mesmo tempo que as removíamos da linha anterior\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mclasses_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m22\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m21\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mitem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclasses_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m21\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#uma das linhas continha uma categoria com informações duplas pois, dependendo da condição que estava aplicada,\n",
    "#o valor seria diferente. assim, criamos uma linha nula logo após e preenchemos ela com as informações extras\n",
    "#ao mesmo tempo que as removíamos da linha anterior\n",
    "classes_info.insert(22, [0,0,0,0,0,0,0,0,0])\n",
    "for k in range(len(classes_info[21])):\n",
    "    item = classes_info[21][k]\n",
    "    try:\n",
    "        classes_info[21][k] = item.split(',')[0].strip(' (Base)')\n",
    "        classes_info[22][k] = item.split(',')[1].strip(' (Mastered)')\n",
    "    except:\n",
    "        if item == 'Onion_Knight':\n",
    "            classes_info[21][k] = item\n",
    "            classes_info[22][k] = f'{item}_Mastered'\n",
    "        else:\n",
    "            classes_info[21][k] = item\n",
    "            classes_info[22][k] = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'int' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d2d46d13c1d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mstat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Low'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mjob\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mstat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Very Low'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mjob\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;34m'%'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstat\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mjob\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'int' is not iterable"
     ]
    }
   ],
   "source": [
    "#também classificamos a taxa de crescimento de atributos por um índice numérico indo de 1 a 5\n",
    "#no lugar de categorias que iam de \"Very Low\" a \"Very High\"\n",
    "#removemos o símbolo de porcentagem dos dados de uma coluna para ela ser totalmente numérica\n",
    "for job in classes_info:\n",
    "    for stat in job:\n",
    "        if stat == 'Very High': job[job.index(stat)] = 5\n",
    "        elif stat == 'High': job[job.index(stat)] = 4\n",
    "        elif stat in ['Medium', 'Average']: job[job.index(stat)] = 3\n",
    "        elif stat == 'Low': job[job.index(stat)] = 2\n",
    "        elif stat == 'Very Low': job[job.index(stat)] = 1\n",
    "        if '%' in stat: job[job.index(stat)] = int(stat.strip('%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-a3c694934d0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'class_name,move_rate,jump_rate,speed,evasion_(%),base_attack,base_magic,base_hitpoints,base_manapoints\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclasses_info\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{item[8].lower()},{item[0]},{item[1]},{item[2]},{item[3]},{item[4]},{item[5]},{item[6]},{item[7]}\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "#por fim imprimimos o arquivo\n",
    "file = open(f'final-fantasy-tactics-classes-info.csv','w+')\n",
    "file.write(f'class_name,move_rate,jump_rate,speed,evasion_(%),base_attack,base_magic,base_hitpoints,base_manapoints\\n')\n",
    "for item in classes_info:\n",
    "    file.write(f'{item[8].lower()},{item[0]},{item[1]},{item[2]},{item[3]},{item[4]},{item[5]},{item[6]},{item[7]}\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acessamos um api que retorna informações do jogo civilization e pedimos as informações de civilizações\n",
    "response = requests.get('https://age-of-empires-2-api.herokuapp.com/api/v1/civilizations')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtramos a parte do json que nos traria as informações de cada civilização\n",
    "civilizations = response.json()\n",
    "civilizations['civilizations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imprimimos as informações que julgamos relevantes\n",
    "file = open('age2.csv', 'w+') \n",
    "file.write('civiliation_name,army_type,unique_unit,unique_tech,team_bonus\\n')\n",
    "for item in civilizations['civilizations']:\n",
    "    file.write(f\"{item['name']},{item['army_type']},{item['unique_unit'][0].split('/')[-1]},{item['unique_tech'][0].split('/')[-1]},{item['team_bonus']}\\n\")\n",
    "file.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acessamos um api que retorna informações de asteróides\n",
    "#pedimos as informações de 100 asteróides com órbita circular ou o mais próximo disso\n",
    "response = requests.get('http://asterank.com/api/asterank?query={\"e\":{\"$lt\":0.1}}&limit=100')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asteroids = response.json()\n",
    "asteroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imprimimos todas as informações desses asteróides. precisamos de um looping para imprimir as chaves como\n",
    "#nomes de coluna e de outro looping para imprimir as informações de cada asteróide\n",
    "file = open('asteroids.csv', 'w+')\n",
    "for title in list(dic.keys()):\n",
    "    file.write(f'{title}')\n",
    "    if title != 'n_dop_obs_used': file.write(',')\n",
    "file.write('\\n')\n",
    "for dic in asterois:\n",
    "    for k in range(len(list(dic.values()))):\n",
    "        file.write(f'{list(dic.values())[k]}')\n",
    "        if k < 81: file.write(',')\n",
    "    file.write('\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acessamos um api que retorna informações sobre futebol e pedimos as informações de competições\n",
    "response = requests.get('http://api.football-data.org/v2/competitions/')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtramos a parte do json que nos traria as informações de cada competição\n",
    "competitions = response.json()\n",
    "competitions['competitions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imprimimos os arquivos. precisamos especificar os casos em que o campeonato ainda não tinha nenhum vencedor\n",
    "#e também o caso em que a região buscada não estava com nenhum campeonato disponível\n",
    "file = open(f'football-data-competitions.csv','w+')\n",
    "file.write(f\"id,area,competition_name,season_start_date,season_end_date,winner\\n\")\n",
    "for dic in competitions['competitions']:\n",
    "    try:\n",
    "        if dic['currentSeason']['winner'] == None: file.write(f\"{dic['id']},{dic['area']['name']},{dic['name']},{dic['currentSeason']['startDate']},{dic['currentSeason']['endDate']},{dic['currentSeason']['winner']}\\n\")\n",
    "        else: file.write(f\"{dic['id']},{dic['area']['name']},{dic['name']},{dic['currentSeason']['startDate']},{dic['currentSeason']['endDate']},{dic['currentSeason']['winner']['name']}\\n\")\n",
    "    except: pass\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nesta parte pedimos as informações de todas as equipes do campeonato \"Série A\" no brasil\n",
    "#era necessário fornecer uma chave de acesso para obter as informações aqui\n",
    "response = requests.get('http://api.football-data.org/v2/competitions/2013/teams', headers={'X-Auth-Token':'46f2a41057a649a59e5412649b2e279b'})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = response.json()\n",
    "teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imprimimos as informações que julgamos relevantes para cada time\n",
    "file = open(f'football-data-teams-brazil.csv','w+')\n",
    "file.write(f\"id,tla,name,founded,venue,website\\n\")\n",
    "for dic in teams['teams']:\n",
    "    file.write(f\"{dic['id']},{dic['tla']},{dic['name']},{dic['founded']},{dic['venue']},{dic['website']}\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#por fim, pedimos todas as informações de todos os times do campeonato \"Série A\" no brasil\n",
    "#e selecionamos apenas as informações de cada jogador. junto a elas colocamos para qual time cada um jogava\n",
    "#para evitar que o site nos bloqueasse pois eram mais de 800 acessos contínuos colocamos um intervalo\n",
    "#de 5 segundos entre cada pedido (tentamos com valores menores que ainda nos prendiam)\n",
    "teams_info = []\n",
    "\n",
    "for team_id in teams['teams']:\n",
    "    response = response.get(f\"http://api.football-data.org/v2/teams/{team_id['id']}\", headers={'X-Auth-Token':'46f2a41057a649a59e5412649b2e279b'})\n",
    "    team_info = response.json()\n",
    "    for dic in team_info['squad']:\n",
    "        teams_info.append([team_id['id'], dic['id'], dic['name'], dic['position'], dic['nationality']])\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#por fim, imprimimos tudo\n",
    "file = open(f'football-data-teams-brazil-squad.csv','w+')\n",
    "file.write(f'team_id,player_id,player_name,player_position,player_country\\n')\n",
    "for item in teams_info:\n",
    "    file.write(f'{item[0]},{item[1]},{item[2]},{item[3]},{item[4]}\\n')\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
