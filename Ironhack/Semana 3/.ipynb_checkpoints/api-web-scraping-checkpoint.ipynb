{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Web Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos classes para todos os métodos de Web Scrapping pois compactam bem o código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scrapper:\n",
    "    def __init__(self, url_pattern, content_parser=None):\n",
    "        self.url_pattern = url_pattern\n",
    "        self.content_parser = content_parser\n",
    "\n",
    "    def scrape_url(self, url):\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code < 300:\n",
    "            result = self.content_parser(response.content)[1:]\n",
    "            self.output_results(result)\n",
    "        else: print(f\"fail. error code: {response.status_code}\")\n",
    "\n",
    "    #para cada categoria do site criamos um arquivo diferente, inclusive para a categoria geral que reúne todas\n",
    "    def output_results(self, r):\n",
    "        if CATEGORY == '': file = open(f'omelete-noticias-geral.csv','w+')\n",
    "        else: file = open(f'omelete-noticias-{CATEGORY[9:]}.csv','w+')\n",
    "        for item in r:\n",
    "            file.write(f'{item}\\n')\n",
    "        file.close()\n",
    "\n",
    "    def kickstart(self):\n",
    "        self.scrape_url(self.url_pattern)\n",
    "\n",
    "\n",
    "#observamos que poderíamos reduzir a área de busca na sopa para evitar retornar lixo\n",
    "def parser(content):\n",
    "    soup = list(bs(content).children)[14]\n",
    "    quoteshtml = soup.find_all('h2')\n",
    "    return [quoteshtml[k].get_text() for k in range(len(quoteshtml))]\n",
    "\n",
    "#utilizamos uma url base que era comum a todas as cinco categorias que buscamos\n",
    "BASE_URL = 'https://www.omelete.com.br/criticas'\n",
    "CATEGORY_LIST = ['', '?section=filmes', '?section=series-tv', '?section=quadrinhos', '?section=musica']\n",
    "URL_PATTERN = ''\n",
    "\n",
    "#utilizamos um looping para rodar por todas as categorias\n",
    "for CATEGORY in CATEGORY_LIST:\n",
    "    URL_PATTERN = f'{BASE_URL}{CATEGORY}'\n",
    "    spider = Scrapper(URL_PATTERN, content_parser=parser)\n",
    "    spider.kickstart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Web Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scrapper:\n",
    "    def __init__(self, url_pattern, content_parser=None):\n",
    "        self.url_pattern = url_pattern\n",
    "        self.content_parser = content_parser\n",
    "\n",
    "    def scrape_url(self, url):\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code < 300:\n",
    "            result = self.content_parser(response.content)\n",
    "            self.output_results(result)\n",
    "        else: print(f\"fail. error code: {response.status_code}\")\n",
    "\n",
    "    def output_results(self, r):\n",
    "        file = open('top100-movies-rt.csv', 'w+')\n",
    "        file.write(f'ranking,rating,movie_name,release_year,total_reviews')\n",
    "        file.write('\\n')\n",
    "        k = 0\n",
    "        for minilist in r:\n",
    "            k += 1\n",
    "            file.write(f'{k},{minilist[0]},{minilist[1]},{minilist[2]},{minilist[3]}\\n')\n",
    "        file.close()\n",
    "\n",
    "    def kickstart(self):\n",
    "        self.scrape_url(self.url_pattern)\n",
    "\n",
    "#reduzimos a área de busca na sopa mas ainda assim percebemos que a lista carregava lixo\n",
    "#por este motivo, limitamos o retorno aos elementos de índice 1 ao -20\n",
    "#desta forma retornamos exatamente a lista de 100 filmes\n",
    "#também procuramos deixar todas as informações numéricas, incluindo porcentagens, em variáveis apenas numéricas\n",
    "def parser(content):\n",
    "    soup = list(list(bs(content).children)[2].children)[3]\n",
    "    infos = thesoup.find_all('tr', class_='')[1:-20]\n",
    "    #cortamos os 13 espaços em branco no começo do título do filme os últimos 7 caracteres referentes ao ano\n",
    "    #colocamos o ano em uma coluna separada\n",
    "    return [[infos[k].find_all('span', class_='tMeterScore')[0].get_text()[1:].strip('%'),\n",
    "            infos[k].find_all('a', class_='unstyled articleLink')[0].get_text()[13:-7],\n",
    "            infos[k].find_all('a', class_='unstyled articleLink')[0].get_text().split(' ')[-1].strip('()'),\n",
    "            infos[k].find_all('td', class_='right hidden-xs')[0].get_text()] for k in range(len(infos))]\n",
    "\n",
    "BASE_URL = 'https://www.rottentomatoes.com/top/bestofrt/'\n",
    "\n",
    "spider = Scrapper(BASE_URL, content_parser=parser)\n",
    "spider.kickstart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Web Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scrapper:\n",
    "    def __init__(self, url_pattern, content_parser=None):\n",
    "        self.url_pattern = url_pattern\n",
    "        self.content_parser = content_parser\n",
    "\n",
    "    def scrape_url(self, url):\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code < 300:\n",
    "            result = self.content_parser(response.content)\n",
    "            self.output_results(result)\n",
    "        else: print(f\"fail. error code: {response.status_code}\")\n",
    "\n",
    "    #como aqui precisávamos limpar as informações mais a fundo preferimos criar uma lista antes de imprimir\n",
    "    #também adicionamos a categoria na lista para termos o nome da personagem nos dados\n",
    "    def output_results(self, r):\n",
    "        infos = r\n",
    "        infos.append(CATEGORY)\n",
    "        classes_info.append(infos)\n",
    "\n",
    "    def kickstart(self):\n",
    "        self.scrape_url(self.url_pattern)\n",
    "\n",
    "#reduzimos a área de busca da sopa para evitar retorno de lixo\n",
    "#percebemos que para três categorias específicas havia um valor a mais e precisamos fazer um retorno diferente\n",
    "def parser(content):\n",
    "    soup = list(list(list(bs(content).children)[2].children)[3].children)[26]\n",
    "    stats = soup.find_all('td')\n",
    "    if CATEGORY not in ['Thief', 'Dragoon', 'Mime']: return [item.get_text().strip(' ').strip('\\n') for item in stats[5:13]]\n",
    "    else: return [item.get_text().strip(' ').strip('\\n') for item in stats[6:14]]\n",
    "\n",
    "classes_info = []\n",
    "#utilizamos uma url base que era comum a todas as categorias que iríamos buscar\n",
    "BASE_URL = 'https://finalfantasy.fandom.com/wiki/'\n",
    "CATEGORY_LIST = ['Squire', 'Chemist', 'Knight', 'Archer', 'White_Mage',\n",
    "                 'Black_Mage', 'Monk', 'Thief', 'Mystic', 'Time_Mage',\n",
    "                 'Geomancer', 'Dragoon', 'Orator', 'Summoner', 'Samurai',\n",
    "                 'Ninja', 'Arithmetician', 'Dancer', 'Bard', 'Mime',\n",
    "                 'Dark_Knight', 'Onion_Knight', 'Holy_Knight', 'Machinist', 'Skyseer',\n",
    "                 'Netherseer', 'Divine_Knight', 'Sword_Saint', 'Templar', 'Dragonkin',\n",
    "                 'Soldier', 'Sky_Pirate', 'Game_Hunter']\n",
    "\n",
    "#utilizamos um looping para rodar todas as categorias que queríamos\n",
    "#mas percebemos que especificamente para 5 categorias o link não seguia o padrão exato dos outros e\n",
    "#precisamos fazer um retorno diferente\n",
    "#por precaução, como eram mais de 30 acessos, colocamos um intervalo de 1 segundo para evitar quaisquer problemas\n",
    "for CATEGORY in CATEGORY_LIST:\n",
    "    if CATEGORY not in ['Arithmetician', 'Skyseer', 'Netherseer', 'Dragonkin', 'Game_Hunter']: URL_PATTERN = f'{BASE_URL}{CATEGORY}_(Tactics)'\n",
    "    else: URL_PATTERN = f'{BASE_URL}{CATEGORY}'\n",
    "    spider = Scrapper(URL_PATTERN, content_parser=parser)\n",
    "    spider.kickstart()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uma das linhas continha uma categoria com informações duplas pois, dependendo da condição que estava aplicada,\n",
    "#o valor seria diferente. assim, criamos uma linha nula logo após e preenchemos ela com as informações extras\n",
    "#ao mesmo tempo que as removíamos da linha anterior\n",
    "classes_info.insert(22, [0,0,0,0,0,0,0,0,0])\n",
    "for k in range(len(classes_info[21])):\n",
    "    item = classes_info[21][k]\n",
    "    try:\n",
    "        classes_info[21][k] = item.split(',')[0].strip(' (Base)')\n",
    "        classes_info[22][k] = item.split(',')[1].strip(' (Mastered)')\n",
    "    except:\n",
    "        if item == 'Onion_Knight':\n",
    "            classes_info[21][k] = item\n",
    "            classes_info[22][k] = f'{item}_Mastered'\n",
    "        else:\n",
    "            classes_info[21][k] = item\n",
    "            classes_info[22][k] = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#também classificamos a taxa de crescimento de atributos por um índice numérico indo de 1 a 5\n",
    "#no lugar de categorias que iam de \"Very Low\" a \"Very High\"\n",
    "#removemos o símbolo de porcentagem dos dados de uma coluna para ela ser totalmente numérica\n",
    "for job in classes_info:\n",
    "    for stat in job:\n",
    "        if stat == 'Very High': job[job.index(stat)] = 5\n",
    "        elif stat == 'High': job[job.index(stat)] = 4\n",
    "        elif stat in ['Medium', 'Average']: job[job.index(stat)] = 3\n",
    "        elif stat == 'Low': job[job.index(stat)] = 2\n",
    "        elif stat == 'Very Low': job[job.index(stat)] = 1\n",
    "        if '%' in stat: job[job.index(stat)] = int(stat.strip('%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#por fim imprimimos o arquivo\n",
    "file = open(f'final-fantasy-tactics-classes-info.csv','w+')\n",
    "file.write(f'class_name,move_rate,jump_rate,speed,evasion_(%),base_attack,base_magic,base_hitpoints,base_manapoints\\n')\n",
    "for item in classes_info:\n",
    "    file.write(f'{item[8].lower()},{item[0]},{item[1]},{item[2]},{item[3]},{item[4]},{item[5]},{item[6]},{item[7]}\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acessamos um api que retorna informações do jogo civilization e pedimos as informações de civilizações\n",
    "response = requests.get('https://age-of-empires-2-api.herokuapp.com/api/v1/civilizations')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtramos a parte do json que nos traria as informações de cada civilização\n",
    "civilizations = response.json()\n",
    "civilizations['civilizations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imprimimos as informações que julgamos relevantes\n",
    "file = open('age2.csv', 'w+') \n",
    "file.write('civiliation_name,army_type,unique_unit,unique_tech,team_bonus\\n')\n",
    "for item in civilizations['civilizations']:\n",
    "    file.write(f\"{item['name']},{item['army_type']},{item['unique_unit'][0].split('/')[-1]},{item['unique_tech'][0].split('/')[-1]},{item['team_bonus']}\\n\")\n",
    "file.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acessamos um api que retorna informações de asteróides\n",
    "#pedimos as informações de 100 asteróides com órbita circular ou o mais próximo disso\n",
    "response = requests.get('http://asterank.com/api/asterank?query={\"e\":{\"$lt\":0.1}}&limit=100')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asteroids = response.json()\n",
    "asteroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imprimimos todas as informações desses asteróides. precisamos de um looping para imprimir as chaves como\n",
    "#nomes de coluna e de outro looping para imprimir as informações de cada asteróide\n",
    "file = open('asteroids.csv', 'w+')\n",
    "for title in list(dic.keys()):\n",
    "    file.write(f'{title}')\n",
    "    if title != 'n_dop_obs_used': file.write(',')\n",
    "file.write('\\n')\n",
    "for dic in asterois:\n",
    "    for k in range(len(list(dic.values()))):\n",
    "        file.write(f'{list(dic.values())[k]}')\n",
    "        if k < 81: file.write(',')\n",
    "    file.write('\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acessamos um api que retorna informações sobre futebol e pedimos as informações de competições\n",
    "response = requests.get('http://api.football-data.org/v2/competitions/')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtramos a parte do json que nos traria as informações de cada competição\n",
    "competitions = response.json()\n",
    "competitions['competitions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imprimimos os arquivos. precisamos especificar os casos em que o campeonato ainda não tinha nenhum vencedor\n",
    "#e também o caso em que a região buscada não estava com nenhum campeonato disponível\n",
    "file = open(f'football-data-competitions.csv','w+')\n",
    "file.write(f\"id,area,competition_name,season_start_date,season_end_date,winner\\n\")\n",
    "for dic in competitions['competitions']:\n",
    "    try:\n",
    "        if dic['currentSeason']['winner'] == None: file.write(f\"{dic['id']},{dic['area']['name']},{dic['name']},{dic['currentSeason']['startDate']},{dic['currentSeason']['endDate']},{dic['currentSeason']['winner']}\\n\")\n",
    "        else: file.write(f\"{dic['id']},{dic['area']['name']},{dic['name']},{dic['currentSeason']['startDate']},{dic['currentSeason']['endDate']},{dic['currentSeason']['winner']['name']}\\n\")\n",
    "    except: pass\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nesta parte pedimos as informações de todas as equipes do campeonato \"Série A\" no brasil\n",
    "#era necessário fornecer uma chave de acesso para obter as informações aqui\n",
    "response = requests.get('http://api.football-data.org/v2/competitions/2013/teams', headers={'X-Auth-Token':'46f2a41057a649a59e5412649b2e279b'})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = response.json()\n",
    "teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imprimimos as informações que julgamos relevantes para cada time\n",
    "file = open(f'football-data-teams-brazil.csv','w+')\n",
    "file.write(f\"id,tla,name,founded,venue,website\\n\")\n",
    "for dic in teams['teams']:\n",
    "    file.write(f\"{dic['id']},{dic['tla']},{dic['name']},{dic['founded']},{dic['venue']},{dic['website']}\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#por fim, pedimos todas as informações de todos os times do campeonato \"Série A\" no brasil\n",
    "#e selecionamos apenas as informações de cada jogador. junto a elas colocamos para qual time cada um jogava\n",
    "#para evitar que o site nos bloqueasse pois eram mais de 800 acessos contínuos colocamos um intervalo\n",
    "#de 5 segundos entre cada pedido (tentamos com valores menores que ainda nos prendiam)\n",
    "teams_info = []\n",
    "\n",
    "for team_id in teams['teams']:\n",
    "    response = response.get(f\"http://api.football-data.org/v2/teams/{team_id['id']}\", headers={'X-Auth-Token':'46f2a41057a649a59e5412649b2e279b'})\n",
    "    team_info = response.json()\n",
    "    for dic in team_info['squad']:\n",
    "        teams_info.append([team_id['id'], dic['id'], dic['name'], dic['position'], dic['nationality']])\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#por fim, imprimimos tudo\n",
    "file = open(f'football-data-teams-brazil-squad.csv','w+')\n",
    "file.write(f'team_id,player_id,player_name,player_position,player_country\\n')\n",
    "for item in teams_info:\n",
    "    file.write(f'{item[0]},{item[1]},{item[2]},{item[3]},{item[4]}\\n')\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
